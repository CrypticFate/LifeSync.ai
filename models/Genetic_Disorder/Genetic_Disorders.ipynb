{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njtGZPPEmilp",
        "outputId": "a23938ce-883a-41f6-ee5a-b26c358f8dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install -q xgboost catboost lightgbm imbalanced-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgOhEGq1mqtN"
      },
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
        "\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUIu4GXsmqqz",
        "outputId": "07f4c2b8-58c8-48aa-fad4-52b9b4bac581"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original data shape: (22083, 45)\n",
            "\n",
            "Disorder Subclass distribution:\n",
            "Disorder Subclass\n",
            "Leigh syndrome                         5160\n",
            "Mitochondrial myopathy                 4405\n",
            "Cystic fibrosis                        3448\n",
            "Tay-Sachs                              2833\n",
            "Diabetes                               1817\n",
            "Hemochromatosis                        1355\n",
            "Leber's hereditary optic neuropathy     648\n",
            "Alzheimer's                             152\n",
            "Cancer                                   97\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class balance ratio: 0.019\n"
          ]
        }
      ],
      "source": [
        "# ==================== LOAD AND EXPLORE DATA ====================\n",
        "train_data = pd.read_csv('train.csv')\n",
        "\n",
        "print(f\"Original data shape: {train_data.shape}\")\n",
        "print(f\"\\nDisorder Subclass distribution:\")\n",
        "print(train_data['Disorder Subclass'].value_counts())\n",
        "print(f\"\\nClass balance ratio: {train_data['Disorder Subclass'].value_counts().min() / train_data['Disorder Subclass'].value_counts().max():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym71ii-TmqoW",
        "outputId": "14f100d6-df22-4bd9-fdb9-9580e36a803e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "DATA PREPROCESSING\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ==================== DATA PREPROCESSING ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dropping irrelevant columns\n",
        "drop_columns = ['Patient Id', 'Patient First Name', 'Family Name', \"Father's name\",\n",
        "                \"Institute Name\", \"Location of Institute\", \"Status\", \"Parental consent\", \"Genetic Disorder\"]\n",
        "train_data = train_data.drop(columns=drop_columns, errors='ignore')\n",
        "\n",
        "# Keep Disorder Subclass as target\n",
        "train_data = train_data.dropna(subset=['Disorder Subclass'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meo0dwAZmqlu",
        "outputId": "75306fc6-1e37-416a-ea5e-ac077e2f2bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Number of classes: 9\n",
            "Classes: [\"Alzheimer's\" 'Cancer' 'Cystic fibrosis' 'Diabetes' 'Hemochromatosis'\n",
            " \"Leber's hereditary optic neuropathy\" 'Leigh syndrome'\n",
            " 'Mitochondrial myopathy' 'Tay-Sachs']\n",
            "\n",
            "Final feature shape: (19915, 44)\n"
          ]
        }
      ],
      "source": [
        "# ==================== ADVANCED FEATURE ENGINEERING ====================\n",
        "\n",
        "# Replacing missing values in numeric columns with median (BETTER than mean for skewed data)\n",
        "numeric_columns = ['No. of previous abortion', 'White Blood cell count (thousand per microliter)',\n",
        "                   'Patient Age', 'Blood cell count (mcL)', \"Mother's age\", \"Father's age\"]\n",
        "for col in numeric_columns:\n",
        "    if col in train_data.columns:\n",
        "        train_data[col] = train_data[col].fillna(train_data[col].median())\n",
        "        train_data[col] = train_data[col].astype('float64')\n",
        "\n",
        "# Create derived features from existing ones\n",
        "if 'Patient Age' in train_data.columns:\n",
        "    train_data['Age_squared'] = train_data['Patient Age'] ** 2\n",
        "    train_data['Age_log'] = np.log1p(train_data['Patient Age'])\n",
        "\n",
        "if \"Mother's age\" in train_data.columns and \"Father's age\" in train_data.columns:\n",
        "    train_data['Parental_age_diff'] = abs(train_data[\"Mother's age\"] - train_data[\"Father's age\"])\n",
        "    train_data['Parental_age_sum'] = train_data[\"Mother's age\"] + train_data[\"Father's age\"]\n",
        "    train_data['Parental_age_avg'] = (train_data[\"Mother's age\"] + train_data[\"Father's age\"]) / 2\n",
        "\n",
        "# Sum of symptoms (important feature)\n",
        "symptom_cols = ['Symptom 1', 'Symptom 2', 'Symptom 3', 'Symptom 4', 'Symptom 5']\n",
        "symptom_cols = [col for col in symptom_cols if col in train_data.columns]\n",
        "if symptom_cols:\n",
        "    for col in symptom_cols:\n",
        "        train_data[col] = train_data[col].fillna(0).astype('int64')\n",
        "    train_data['Total_symptoms'] = train_data[symptom_cols].sum(axis=1)\n",
        "    train_data['Symptoms_ratio'] = train_data['Total_symptoms'] / len(symptom_cols)\n",
        "\n",
        "# Blood cell ratio (important clinical feature)\n",
        "if 'Blood cell count (mcL)' in train_data.columns and 'White Blood cell count (thousand per microliter)' in train_data.columns:\n",
        "    train_data['WBC_RBC_ratio'] = train_data['White Blood cell count (thousand per microliter)'] / (train_data['Blood cell count (mcL)'] + 1e-6)\n",
        "\n",
        "# Replacing missing values in categorical columns with 'Unknown'\n",
        "categorical_columns = ['Blood test result', 'Birth defects', 'Gender',\n",
        "                       'Heart Rate (rates/min', 'Respiratory Rate (breaths/min)',\n",
        "                       'Follow-up', 'Place of birth']\n",
        "categorical_columns = [col for col in categorical_columns if col in train_data.columns]\n",
        "train_data[categorical_columns] = train_data[categorical_columns].fillna('Unknown')\n",
        "\n",
        "# Replacing various 'NA' and similar values with 'No'\n",
        "train_data = train_data.replace([\"-\", \"Not applicable\", \"Not available\", \"None\", \"No record\"], \"No\")\n",
        "\n",
        "# Converting binary categorical variables to 0 and 1\n",
        "binary_var = [\"Maternal gene\", \"Genes in mother's side\", 'Inherited from father', 'Paternal gene',\n",
        "              'Birth asphyxia', 'Folic acid details (peri-conceptional)',\n",
        "              'H/O serious maternal illness', 'H/O radiation exposure (x-ray)',\n",
        "              'H/O substance abuse', 'Assisted conception IVF/ART',\n",
        "              'History of anomalies in previous pregnancies',\n",
        "              'Autopsy shows birth defect (if applicable)']\n",
        "binary_var = [col for col in binary_var if col in train_data.columns]\n",
        "for col in binary_var:\n",
        "    train_data[col] = train_data[col].replace({\"Yes\": 1, \"No\": 0})\n",
        "    train_data[col] = train_data[col].fillna(0).astype('int64')\n",
        "\n",
        "# Create binary inheritance pattern feature\n",
        "genetic_features = [col for col in binary_var if 'gene' in col.lower() or 'inherited' in col.lower()]\n",
        "if genetic_features:\n",
        "    train_data['Genetic_inheritance_score'] = train_data[genetic_features].sum(axis=1)\n",
        "\n",
        "# Encode categorical columns\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    if col in train_data.columns:\n",
        "        le = LabelEncoder()\n",
        "        train_data[col] = le.fit_transform(train_data[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# Target encoding\n",
        "label_encoder_target = LabelEncoder()\n",
        "y_encoded = label_encoder_target.fit_transform(train_data['Disorder Subclass'])\n",
        "num_classes = len(label_encoder_target.classes_)\n",
        "\n",
        "print(f\"\\nNumber of classes: {num_classes}\")\n",
        "print(f\"Classes: {label_encoder_target.classes_}\")\n",
        "\n",
        "# Features\n",
        "X = train_data.drop('Disorder Subclass', axis=1)\n",
        "print(f\"\\nFinal feature shape: {X.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVTV1bynmqjM",
        "outputId": "b292cfd8-838d-4deb-8dc2-fb4c3783f12a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "HANDLING CLASS IMBALANCE WITH SMOTE\n",
            "============================================================\n",
            "\n",
            "Balanced data shape: (46440, 44)\n",
            "Balanced class distribution:\n",
            "  Class 0: 5160 samples\n",
            "  Class 1: 5160 samples\n",
            "  Class 2: 5160 samples\n",
            "  Class 3: 5160 samples\n",
            "  Class 4: 5160 samples\n",
            "  Class 5: 5160 samples\n",
            "  Class 6: 5160 samples\n",
            "  Class 7: 5160 samples\n",
            "  Class 8: 5160 samples\n",
            "\n",
            "Train set size: (37152, 44)\n",
            "Test set size: (9288, 44)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HANDLING CLASS IMBALANCE WITH SMOTE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Impute any remaining NaNs in X before SMOTE\n",
        "# Using median imputation as a robust choice for numerical features\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# Use SMOTE to handle imbalanced classes\n",
        "smote = SMOTE(random_state=42, k_neighbors=5)\n",
        "X_balanced, y_balanced = smote.fit_resample(X, y_encoded)\n",
        "\n",
        "print(f\"\\nBalanced data shape: {X_balanced.shape}\")\n",
        "print(f\"Balanced class distribution:\")\n",
        "unique, counts = np.unique(y_balanced, return_counts=True)\n",
        "for u, c in zip(unique, counts):\n",
        "    print(f\"  Class {u}: {c} samples\")\n",
        "\n",
        "# Split with balanced data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4MLGuWVnwjO"
      },
      "outputs": [],
      "source": [
        "# ==================== FEATURE SCALING ====================\n",
        "# Use RobustScaler for better handling of outliers\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQwSQjJAmqga",
        "outputId": "e6b25d6d-4607-4ac8-f1ff-6567d7545030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING OPTIMIZED XGBOOST\n",
            "============================================================\n",
            "XGBoost Accuracy: 0.6693\n",
            "XGBoost F1-Score (weighted): 0.6669\n"
          ]
        }
      ],
      "source": [
        "# ==================== MODEL 1: OPTIMIZED XGBOOST ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING OPTIMIZED XGBOOST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    colsample_bylevel=0.8,\n",
        "    gamma=1.0,\n",
        "    min_child_weight=1,\n",
        "    objective='multi:softmax',\n",
        "    num_class=num_classes,\n",
        "    random_state=42,\n",
        "    tree_method='hist',\n",
        "    eval_metric='mlogloss',\n",
        "    verbosity=0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train, verbose=False)\n",
        "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
        "xgb_acc = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb, average='weighted')\n",
        "\n",
        "print(f\"XGBoost Accuracy: {xgb_acc:.4f}\")\n",
        "print(f\"XGBoost F1-Score (weighted): {xgb_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RVsJZR-mqd5",
        "outputId": "c3d27e83-722d-4933-bed3-48f38e2b0ef1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING LIGHTGBM\n",
            "============================================================\n",
            "LightGBM Accuracy: 0.6844\n",
            "LightGBM F1-Score (weighted): 0.6837\n"
          ]
        }
      ],
      "source": [
        "# ==================== MODEL 2: LIGHTGBM ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING LIGHTGBM\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    min_child_samples=20,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "lgb_model.fit(X_train_scaled, y_train)\n",
        "y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
        "lgb_acc = accuracy_score(y_test, y_pred_lgb)\n",
        "lgb_f1 = f1_score(y_test, y_pred_lgb, average='weighted')\n",
        "\n",
        "print(f\"LightGBM Accuracy: {lgb_acc:.4f}\")\n",
        "print(f\"LightGBM F1-Score (weighted): {lgb_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_0GPoTImqbT",
        "outputId": "e7907538-189d-4863-a107-99c0d08f8405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING CATBOOST\n",
            "============================================================\n",
            "CatBoost Accuracy: 0.6501\n",
            "CatBoost F1-Score (weighted): 0.6466\n"
          ]
        }
      ],
      "source": [
        "# ==================== MODEL 3: CATBOOST ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING CATBOOST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "cat_model = CatBoostClassifier(\n",
        "    iterations=500,\n",
        "    depth=7,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    bootstrap_type='Bernoulli', # Changed to Bernoulli\n",
        "    random_state=42,\n",
        "    verbose=False,\n",
        "    task_type='CPU'\n",
        ")\n",
        "\n",
        "cat_model.fit(X_train_scaled, y_train, verbose=False)\n",
        "y_pred_cat = cat_model.predict(X_test_scaled)\n",
        "cat_acc = accuracy_score(y_test, y_pred_cat)\n",
        "cat_f1 = f1_score(y_test, y_pred_cat, average='weighted')\n",
        "\n",
        "print(f\"CatBoost Accuracy: {cat_acc:.4f}\")\n",
        "print(f\"CatBoost F1-Score (weighted): {cat_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF_Z2j-SoMvn",
        "outputId": "aaf9d7e2-93ac-45c8-e42e-aa705bf06de3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING RANDOM FOREST\n",
            "============================================================\n",
            "Random Forest Accuracy: 0.6760\n",
            "Random Forest F1-Score (weighted): 0.6717\n"
          ]
        }
      ],
      "source": [
        "# ==================== MODEL 4: RANDOM FOREST ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING RANDOM FOREST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=15,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train_scaled, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test_scaled)\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf, average='weighted')\n",
        "\n",
        "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
        "print(f\"Random Forest F1-Score (weighted): {rf_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "UbkB0aQ-mqYq",
        "outputId": "f653bb55-11c2-469a-c6d8-ef018f10eb82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING IMPROVED NEURAL NETWORK\n",
            "============================================================\n",
            "\n",
            "Neural Network Architecture:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">44</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">23,040</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m44\u001b[0m)             │           \u001b[38;5;34m176\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │        \u001b[38;5;34m23,040\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m)              │           \u001b[38;5;34m297\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">201,913</span> (788.72 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m201,913\u001b[0m (788.72 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">199,905</span> (780.88 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m199,905\u001b[0m (780.88 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,008</span> (7.84 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,008\u001b[0m (7.84 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Neural Network...\n",
            "Neural Network Accuracy: 0.5964\n",
            "Neural Network F1-Score (weighted): 0.5918\n"
          ]
        }
      ],
      "source": [
        "# ==================== MODEL 5: NEURAL NETWORK (IMPROVED) ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING IMPROVED NEURAL NETWORK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_train_onehot = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test_onehot = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "nn_model = keras.Sequential([\n",
        "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
        "\n",
        "    # Batch norm first\n",
        "    layers.BatchNormalization(),\n",
        "\n",
        "    # Dense layers with decreasing units\n",
        "    layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.4),\n",
        "\n",
        "    layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.0005)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Dense(32, activation='relu'),\n",
        "\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "nn_model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\nNeural Network Architecture:\")\n",
        "nn_model.summary()\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss', patience=20, restore_best_weights=True, verbose=0\n",
        ")\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7, verbose=0\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Neural Network...\")\n",
        "history = nn_model.fit(\n",
        "    X_train_scaled, y_train_onehot,\n",
        "    validation_data=(X_test_scaled, y_test_onehot),\n",
        "    epochs=150,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping, reduce_lr],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "y_pred_nn_prob = nn_model.predict(X_test_scaled, verbose=0)\n",
        "y_pred_nn = np.argmax(y_pred_nn_prob, axis=1)\n",
        "nn_acc = accuracy_score(y_test, y_pred_nn)\n",
        "nn_f1 = f1_score(y_test, y_pred_nn, average='weighted')\n",
        "\n",
        "print(f\"Neural Network Accuracy: {nn_acc:.4f}\")\n",
        "print(f\"Neural Network F1-Score (weighted): {nn_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uXi2MJMtmqWI",
        "outputId": "a16b9881-cd44-4d7e-a35b-04825d48bc6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "CREATING WEIGHTED VOTING ENSEMBLE\n",
            "============================================================\n",
            "\n",
            "Ensemble Weights:\n",
            "  XGBoost: 0.204\n",
            "  LightGBM: 0.209\n",
            "  CatBoost: 0.198\n",
            "  Random Forest: 0.206\n",
            "  Neural Network: 0.182\n",
            "\n",
            "Ensemble Accuracy: 0.6762\n",
            "Ensemble F1-Score (weighted): 0.6739\n"
          ]
        }
      ],
      "source": [
        "# ==================== WEIGHTED VOTING ENSEMBLE ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CREATING WEIGHTED VOTING ENSEMBLE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get probabilities from all models\n",
        "y_pred_xgb_prob = xgb_model.predict_proba(X_test_scaled)\n",
        "y_pred_lgb_prob = lgb_model.predict_proba(X_test_scaled)\n",
        "y_pred_cat_prob = cat_model.predict_proba(X_test_scaled)\n",
        "y_pred_rf_prob = rf_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Weighted ensemble - normalize by accuracy\n",
        "total_acc = xgb_acc + lgb_acc + cat_acc + rf_acc + nn_acc\n",
        "w_xgb = xgb_acc / total_acc\n",
        "w_lgb = lgb_acc / total_acc\n",
        "w_cat = cat_acc / total_acc\n",
        "w_rf = rf_acc / total_acc\n",
        "w_nn = nn_acc / total_acc\n",
        "\n",
        "ensemble_prob = (w_xgb * y_pred_xgb_prob +\n",
        "                 w_lgb * y_pred_lgb_prob +\n",
        "                 w_cat * y_pred_cat_prob +\n",
        "                 w_rf * y_pred_rf_prob +\n",
        "                 w_nn * y_pred_nn_prob)\n",
        "\n",
        "y_pred_ensemble = np.argmax(ensemble_prob, axis=1)\n",
        "ensemble_acc = accuracy_score(y_test, y_pred_ensemble)\n",
        "ensemble_f1 = f1_score(y_test, y_pred_ensemble, average='weighted')\n",
        "\n",
        "print(f\"\\nEnsemble Weights:\")\n",
        "print(f\"  XGBoost: {w_xgb:.3f}\")\n",
        "print(f\"  LightGBM: {w_lgb:.3f}\")\n",
        "print(f\"  CatBoost: {w_cat:.3f}\")\n",
        "print(f\"  Random Forest: {w_rf:.3f}\")\n",
        "print(f\"  Neural Network: {w_nn:.3f}\")\n",
        "\n",
        "print(f\"\\nEnsemble Accuracy: {ensemble_acc:.4f}\")\n",
        "print(f\"Ensemble F1-Score (weighted): {ensemble_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3oSJcoyimqTT",
        "outputId": "8458f12a-aabc-4303-d336-b43e7a2ef35d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL RESULTS SUMMARY\n",
            "============================================================\n",
            "\n",
            "                 Accuracy  F1-Score\n",
            "XGBoost         0.669251  0.666861\n",
            "LightGBM        0.684432  0.683689\n",
            "CatBoost        0.650086  0.646644\n",
            "Random Forest   0.676034  0.671695\n",
            "Neural Network  0.596361  0.591784\n",
            "Ensemble        0.676249  0.673902\n",
            "\n",
            "🏆 BEST MODEL: LightGBM with 0.6844 accuracy\n"
          ]
        }
      ],
      "source": [
        "# ==================== RESULTS SUMMARY ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "results = {\n",
        "    'XGBoost': {'Accuracy': xgb_acc, 'F1-Score': xgb_f1},\n",
        "    'LightGBM': {'Accuracy': lgb_acc, 'F1-Score': lgb_f1},\n",
        "    'CatBoost': {'Accuracy': cat_acc, 'F1-Score': cat_f1},\n",
        "    'Random Forest': {'Accuracy': rf_acc, 'F1-Score': rf_f1},\n",
        "    'Neural Network': {'Accuracy': nn_acc, 'F1-Score': nn_f1},\n",
        "    'Ensemble': {'Accuracy': ensemble_acc, 'F1-Score': ensemble_f1}\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\n\", results_df)\n",
        "\n",
        "print(f\"\\n🏆 BEST MODEL: {results_df['Accuracy'].idxmax()} with {results_df['Accuracy'].max():.4f} accuracy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Cg_kHxuFmqEK",
        "outputId": "ce890408-50b5-4e2f-8ce2-f9be3f6eaae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "DETAILED CLASSIFICATION REPORT - ENSEMBLE MODEL\n",
            "============================================================\n",
            "                                     precision    recall  f1-score   support\n",
            "\n",
            "                        Alzheimer's       0.96      1.00      0.98      1032\n",
            "                             Cancer       0.95      1.00      0.97      1032\n",
            "                    Cystic fibrosis       0.56      0.51      0.53      1032\n",
            "                           Diabetes       0.70      0.62      0.66      1032\n",
            "                    Hemochromatosis       0.70      0.73      0.72      1032\n",
            "Leber's hereditary optic neuropathy       0.85      0.87      0.86      1032\n",
            "                     Leigh syndrome       0.44      0.50      0.46      1032\n",
            "             Mitochondrial myopathy       0.40      0.37      0.38      1032\n",
            "                          Tay-Sachs       0.50      0.50      0.50      1032\n",
            "\n",
            "                           accuracy                           0.68      9288\n",
            "                          macro avg       0.67      0.68      0.67      9288\n",
            "                       weighted avg       0.67      0.68      0.67      9288\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==================== DETAILED CLASSIFICATION REPORT ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DETAILED CLASSIFICATION REPORT - ENSEMBLE MODEL\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred_ensemble,\n",
        "                          target_names=label_encoder_target.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5UMguqYroVOX",
        "outputId": "32a5b321-71b1-4834-f29f-0f3050d80018"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "TOP 20 IMPORTANT FEATURES\n",
            "============================================================\n",
            "\n",
            "XGBoost Feature Importance (Top 20):\n",
            "                                         Feature  Importance\n",
            "40                                Total_symptoms    0.272031\n",
            "41                                Symptoms_ratio    0.140778\n",
            "43                     Genetic_inheritance_score    0.050618\n",
            "34                                     Symptom 5    0.033867\n",
            "1                         Genes in mother's side    0.027679\n",
            "33                                     Symptom 4    0.019631\n",
            "30                                     Symptom 1    0.019580\n",
            "2                          Inherited from father    0.019259\n",
            "32                                     Symptom 3    0.018596\n",
            "31                                     Symptom 2    0.017309\n",
            "4                                  Paternal gene    0.017261\n",
            "3                                  Maternal gene    0.017073\n",
            "36                                       Age_log    0.015887\n",
            "25  History of anomalies in previous pregnancies    0.015574\n",
            "20        Folic acid details (peri-conceptional)    0.015343\n",
            "24                   Assisted conception IVF/ART    0.015113\n",
            "35                                   Age_squared    0.014857\n",
            "16                                        Gender    0.014847\n",
            "0                                    Patient Age    0.014595\n",
            "18    Autopsy shows birth defect (if applicable)    0.014170\n"
          ]
        }
      ],
      "source": [
        "# ==================== FEATURE IMPORTANCE ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TOP 20 IMPORTANT FEATURES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "feature_importance_xgb = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nXGBoost Feature Importance (Top 20):\")\n",
        "print(feature_importance_xgb.head(20))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}